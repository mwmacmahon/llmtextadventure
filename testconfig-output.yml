app_name: Chat
config:
  llm_config:
    backend_config:
      hftgi_settings:
        model: llama2
      name_of_model: llama3
    backend_config_type: HFTGI
    generation_config:
      max_tokens: 500
      temperature: 0.5
      top_p: null
    generation_preset: default
  parsing_config:
    parsing_functions: []
  transformation_config:
    transformation_functions: []
state:
  hidden_data: null
  history: []
  raw_history: []
  situational_flags: null
