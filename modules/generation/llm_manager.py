

from typing import Type, TypeVar, Optional, Union, Any, Dict, get_args, get_origin, get_type_hints
from types import NoneType
from pydantic import BaseModel, ValidationError, model_validator, field_validator, validator
import tiktoken

from modules.core.config import BaseConfig
from modules.generation.backend_patterns import Backend, BackendConfig, BACKEND_CLASSES
from modules.generation.generation_patterns import GenerationConfig
from modules.generation.llm_patterns import LLMConfig

# All possible classes must be importable here
from modules.generation.backends.openai import OpenAIBackend
from modules.generation.backends.hftgi import HFTGIBackend
from modules.generation.backends.oobabooga import OobaboogaBackend

# Initialize console logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMManager:
    """
    LLMManager handles communication with Language Model (LLM) services.
    It is responsible for sending prompts and processing responses.

    Does not use schema, as it is not a configuration class.

    Attributes:
        llm_config (LLMConfig): Configuration for the LLM service.
    """
    llm_config: LLMConfig
    backend: Backend  # Must be one of the ____Backend classes


    def __init__(self, llm_config: LLMConfig):
        """
        Initializes the LLMManager with the given LLM configuration.

        Args:
            llm_config (LLMConfig): The configuration for the LLM service.
        """
        self.llm_config = llm_config
        if llm_config.backend_config_type not in BACKEND_CLASSES.keys():
            raise ValueError(
                f"Invalid backend type: {llm_config.backend_config_type}. Valid values are {BACKEND_CLASSES.keys()}"
            )
        try:
            # print(f"Found backend type: {llm_config.backend_config_type}")
            backend_class = globals()[BACKEND_CLASSES[llm_config.backend_config_type]]
            # print(f"Found backend class: {backend_class}")
        except Exception as e:
            raise ValueError(
                f"Could not find class: {BACKEND_CLASSES[llm_config.backend_config_type]}. Is it imported in modules.generation.llmengine?"
            )
        try:
            self.backend = backend_class(llm_config.backend_config)
        except Exception as e:
            raise Exception(
                f"Could not create class: {backend_class} with config {llm_config.backend_config}."
            )


    def count_tokens_tiktoken(self, text: str, encoding_name: str = "cl100k_base") -> int:
        """
        Count the number of tokens in a text string using tiktoken.

        Args:
            text (str): The text string to count tokens for.
            encoding_name (str): The encoding name to use with tiktoken.

        Returns:
            int: The number of tokens in the text.
        """
        encoding = tiktoken.get_encoding(encoding_name)
        return len(encoding.encode(text))

    def count_tokens(self, text: str) -> int:
        """
        Count the number of tokens in a text string.

        Args:
            text (str): The text string to count tokens for.

        Returns:
            int: The number of tokens in the text.
        """
        return self.count_tokens_tiktoken(text)

    def generate_response(self, prompt: str, chat_history: list, prefix: str = None):
        """
        Generates a response from the LLM based on the given prompt.z

        Args:
            prompt (str): The prompt to send to the LLM.
            chat_history (list): The chat history to send to the LLM.
            prefix (str, optional): The prefix to send to the LLM.

        Returns:
            str: The response generated by the LLM.
        """
        return self.backend.generate_response(prompt, chat_history, self.llm_config.generation_config, prefix)

